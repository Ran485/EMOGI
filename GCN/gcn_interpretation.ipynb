{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret Results of Trained GCN\n",
    "In this notebook, I'll try to visualize the learning of a trained GCN model. This should involve the following steps:\n",
    "* Interpret the predictions made by the model\n",
    "* Visualize the filters of the Graph Convolution\n",
    "* Do a TSNE plot of the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import my_gcn\n",
    "import random, h5py\n",
    "import tensorflow as tf\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from gcn.utils import *\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "import seaborn\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../GCN'))\n",
    "from my_gcn import MYGCN\n",
    "import math\n",
    "bestSplit = lambda x: (round(math.sqrt(x)), math.ceil(x / round(math.sqrt(x))))\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dir = '../data/GCN/training/2018_01_22_17_10_15/'\n",
    "data_file = '../data/simulation/simulated_input_legionella.h5'\n",
    "CLASSIFICATION_THRESHOLD = 0.5\n",
    "\n",
    "with h5py.File(data_file, 'r') as f:\n",
    "    network = f['network'][:]\n",
    "    features = f['features'][:]\n",
    "    node_names = f['gene_names'][:]\n",
    "    y_train = f['y_train'][:]\n",
    "    y_test = f['y_test'][:]\n",
    "    if 'y_val' in f:\n",
    "        y_val = f['y_val'][:]\n",
    "    else:\n",
    "        y_val = None\n",
    "    train_mask = f['mask_train'][:]\n",
    "    test_mask = f['mask_test'][:]\n",
    "    if 'mask_val' in f:\n",
    "        val_mask = f['mask_val'][:]\n",
    "    else:\n",
    "        val_mask = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "I want to see what the GCN predicts and if those predictions make any sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name       518671.000\n",
      "Prob_pos      530.002\n",
      "Prob_neg      488.998\n",
      "dtype: float64\n",
      "Predicted 1018 genes of 1019 total to be involved in infection\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Prob_pos</th>\n",
       "      <th>Prob_neg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128.000</th>\n",
       "      <td>128.000</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574.000</th>\n",
       "      <td>574.000</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325.000</th>\n",
       "      <td>325.000</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98.000</th>\n",
       "      <td>98.000</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848.000</th>\n",
       "      <td>848.000</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819.000</th>\n",
       "      <td>819.000</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863.000</th>\n",
       "      <td>863.000</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381.000</th>\n",
       "      <td>381.000</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120.000</th>\n",
       "      <td>120.000</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718.000</th>\n",
       "      <td>718.000</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750.000</th>\n",
       "      <td>750.000</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970.000</th>\n",
       "      <td>970.000</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293.000</th>\n",
       "      <td>293.000</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155.000</th>\n",
       "      <td>155.000</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568.000</th>\n",
       "      <td>568.000</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726.000</th>\n",
       "      <td>726.000</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590.000</th>\n",
       "      <td>590.000</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216.000</th>\n",
       "      <td>216.000</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358.000</th>\n",
       "      <td>358.000</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17.000</th>\n",
       "      <td>17.000</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name  Prob_pos  Prob_neg\n",
       "ID                                 \n",
       "128.000 128.000     0.581     0.419\n",
       "574.000 574.000     0.572     0.428\n",
       "325.000 325.000     0.564     0.436\n",
       "98.000   98.000     0.562     0.438\n",
       "848.000 848.000     0.561     0.439\n",
       "819.000 819.000     0.555     0.445\n",
       "863.000 863.000     0.553     0.447\n",
       "381.000 381.000     0.553     0.447\n",
       "120.000 120.000     0.548     0.452\n",
       "718.000 718.000     0.548     0.452\n",
       "750.000 750.000     0.547     0.453\n",
       "970.000 970.000     0.546     0.454\n",
       "293.000 293.000     0.546     0.454\n",
       "155.000 155.000     0.545     0.455\n",
       "568.000 568.000     0.545     0.455\n",
       "726.000 726.000     0.543     0.457\n",
       "590.000 590.000     0.543     0.457\n",
       "216.000 216.000     0.543     0.457\n",
       "358.000 358.000     0.543     0.457\n",
       "17.000   17.000     0.543     0.457"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load predictions\n",
    "predictions = pd.DataFrame.from_csv(model_dir + '/predictions.tsv', sep='\\t', header=0)\n",
    "print (predictions.sum(axis=0))\n",
    "pos_predicted = predictions[predictions.Prob_pos > CLASSIFICATION_THRESHOLD]\n",
    "\n",
    "# add columns to distinguish between training and test set\n",
    "labels_df = pd.DataFrame(node_names, index=node_names[:, 0], columns=['ID', 'name']).drop('ID', axis=1)\n",
    "labels_df['label'] = (y_train[:,0] | y_test[:,0])\n",
    "labels_df['train_label'] = y_train[:, 0]\n",
    "labels_df['test_label'] = y_test[:, 0]\n",
    "\n",
    "# show the ones that are most confidently predicted\n",
    "print (\"Predicted {} genes of {} total to be involved in infection\".format(pos_predicted.shape[0], predictions.shape[0]))\n",
    "pos_predicted.sort_values(by='Prob_pos', ascending=False).to_csv(model_dir + '/positive_prediction.txt', sep='\\t')\n",
    "pos_predicted.sort_values(by='Prob_pos', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 38 out of 38 known infection genes\n",
      "Out of these 38, 11 were not shown during training.\n",
      "Predicted 11 out of 11 test genes (100.0%)\n",
      "Predicted 27 out of 27 train genes (100.0%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "      <th>train_label</th>\n",
       "      <th>test_label</th>\n",
       "      <th>Name</th>\n",
       "      <th>Prob_pos</th>\n",
       "      <th>Prob_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>877.000</th>\n",
       "      <td>877.000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>877.000</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635.000</th>\n",
       "      <td>635.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>635.000</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816.000</th>\n",
       "      <td>816.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>816.000</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199.000</th>\n",
       "      <td>199.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>199.000</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442.000</th>\n",
       "      <td>442.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>442.000</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503.000</th>\n",
       "      <td>503.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>503.000</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370.000</th>\n",
       "      <td>370.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>370.000</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153.000</th>\n",
       "      <td>153.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>153.000</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411.000</th>\n",
       "      <td>411.000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>411.000</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296.000</th>\n",
       "      <td>296.000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>296.000</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80.000</th>\n",
       "      <td>80.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>80.000</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432.000</th>\n",
       "      <td>432.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>432.000</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112.000</th>\n",
       "      <td>112.000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>112.000</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472.000</th>\n",
       "      <td>472.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>472.000</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389.000</th>\n",
       "      <td>389.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>389.000</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452.000</th>\n",
       "      <td>452.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>452.000</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530.000</th>\n",
       "      <td>530.000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>530.000</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162.000</th>\n",
       "      <td>162.000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>162.000</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327.000</th>\n",
       "      <td>327.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>327.000</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332.000</th>\n",
       "      <td>332.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>332.000</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119.000</th>\n",
       "      <td>119.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>119.000</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266.000</th>\n",
       "      <td>266.000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>266.000</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135.000</th>\n",
       "      <td>135.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>135.000</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207.000</th>\n",
       "      <td>207.000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>207.000</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715.000</th>\n",
       "      <td>715.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>715.000</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39.000</th>\n",
       "      <td>39.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>39.000</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549.000</th>\n",
       "      <td>549.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>549.000</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186.000</th>\n",
       "      <td>186.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>186.000</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.000</th>\n",
       "      <td>2.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275.000</th>\n",
       "      <td>275.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>275.000</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395.000</th>\n",
       "      <td>395.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>395.000</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515.000</th>\n",
       "      <td>515.000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>515.000</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359.000</th>\n",
       "      <td>359.000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>359.000</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740.000</th>\n",
       "      <td>740.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>740.000</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900.000</th>\n",
       "      <td>900.000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>900.000</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226.000</th>\n",
       "      <td>226.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>226.000</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924.000</th>\n",
       "      <td>924.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>924.000</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988.000</th>\n",
       "      <td>988.000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>988.000</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name  label  train_label  test_label    Name  Prob_pos  Prob_neg\n",
       "877.000 877.000      1        False           1 877.000     0.539     0.461\n",
       "635.000 635.000      1         True           0 635.000     0.536     0.464\n",
       "816.000 816.000      1         True           0 816.000     0.534     0.466\n",
       "199.000 199.000      1         True           0 199.000     0.532     0.468\n",
       "442.000 442.000      1         True           0 442.000     0.530     0.470\n",
       "503.000 503.000      1         True           0 503.000     0.528     0.472\n",
       "370.000 370.000      1         True           0 370.000     0.528     0.472\n",
       "153.000 153.000      1         True           0 153.000     0.525     0.475\n",
       "411.000 411.000      1        False           1 411.000     0.524     0.476\n",
       "296.000 296.000      1        False           1 296.000     0.524     0.476\n",
       "80.000   80.000      1         True           0  80.000     0.523     0.477\n",
       "432.000 432.000      1         True           0 432.000     0.522     0.478\n",
       "112.000 112.000      1        False           1 112.000     0.521     0.479\n",
       "472.000 472.000      1         True           0 472.000     0.521     0.479\n",
       "389.000 389.000      1         True           0 389.000     0.521     0.479\n",
       "452.000 452.000      1         True           0 452.000     0.521     0.479\n",
       "530.000 530.000      1        False           1 530.000     0.520     0.480\n",
       "162.000 162.000      1        False           1 162.000     0.520     0.480\n",
       "327.000 327.000      1         True           0 327.000     0.519     0.481\n",
       "332.000 332.000      1         True           0 332.000     0.519     0.481\n",
       "119.000 119.000      1         True           0 119.000     0.519     0.481\n",
       "266.000 266.000      1        False           1 266.000     0.519     0.481\n",
       "135.000 135.000      1         True           0 135.000     0.519     0.481\n",
       "207.000 207.000      1        False           1 207.000     0.518     0.482\n",
       "715.000 715.000      1         True           0 715.000     0.518     0.482\n",
       "39.000   39.000      1         True           0  39.000     0.518     0.482\n",
       "549.000 549.000      1         True           0 549.000     0.517     0.483\n",
       "186.000 186.000      1         True           0 186.000     0.517     0.483\n",
       "2.000     2.000      1         True           0   2.000     0.516     0.484\n",
       "275.000 275.000      1         True           0 275.000     0.516     0.484\n",
       "395.000 395.000      1         True           0 395.000     0.516     0.484\n",
       "515.000 515.000      1        False           1 515.000     0.515     0.485\n",
       "359.000 359.000      1        False           1 359.000     0.515     0.485\n",
       "740.000 740.000      1         True           0 740.000     0.515     0.485\n",
       "900.000 900.000      1        False           1 900.000     0.515     0.485\n",
       "226.000 226.000      1         True           0 226.000     0.515     0.485\n",
       "924.000 924.000      1         True           0 924.000     0.514     0.486\n",
       "988.000 988.000      1         True           0 988.000     0.513     0.487"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_with_pred = labels_df.join(predictions)\n",
    "predictions_for_knowns = labels_with_pred[labels_with_pred.label == 1]\n",
    "pos_pred = predictions_for_knowns[predictions_for_knowns.Prob_pos > CLASSIFICATION_THRESHOLD]\n",
    "known_pos_predictions = pos_pred.shape[0]\n",
    "number_of_knowns = predictions_for_knowns.shape[0]\n",
    "pred_test = pos_pred[pos_pred.test_label == 1]\n",
    "pred_train = pos_pred[pos_pred.train_label == 1]\n",
    "\n",
    "print (\"Predicted {} out of {} known infection genes\".format(known_pos_predictions,\n",
    "                                                             number_of_knowns)\n",
    "      )\n",
    "print (\"Out of these {}, {} were not shown during training.\".format(number_of_knowns,\n",
    "                                                                    y_test[:, 0].sum())\n",
    "      )\n",
    "print (\"Predicted {} out of {} test genes ({}%)\".format(pred_test.shape[0],\n",
    "                                                        y_test[:,0].sum(),\n",
    "                                                        pred_test.shape[0]/y_test[:,0].sum()*100.)\n",
    "      )\n",
    "print (\"Predicted {} out of {} train genes ({}%)\".format(pred_train.shape[0],\n",
    "                                                         y_train[:,0].sum(),\n",
    "                                                         pred_train.shape[0]/y_train[:,0].sum()*100.)\n",
    "      )\n",
    "predictions_for_knowns.sort_values(by='Prob_pos', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1019, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0MAAAHwCAYAAABkAbQdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMAQAAAAgJP1/j+N8xj/BMGUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f125844a9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ROC curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "y = np.logical_or(y_train, y_test)\n",
    "print (y.shape)\n",
    "fpr, tpr, thresholds = roc_curve(y[:, 0], predictions.Prob_pos)\n",
    "roc_auc = roc_auc_score(y[:, 0], predictions.Prob_pos)\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "plt.plot(fpr, tpr, lw=3, label='AUC = {0:.2f}'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=3, linestyle='--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Simulated Dataset (38 Cliques of Size 5)')\n",
    "plt.legend(loc='lower right')\n",
    "fig.savefig(model_dir + 'roc_curve.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0MAAAHwCAYAAABkAbQdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSJIkSVIvGYYkSZIk9dJ/ARvZFNs8oDsYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc427d79400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "pr, rec, thresholds = precision_recall_curve(y[:, 0], predictions.Prob_pos)\n",
    "aupr = average_precision_score(y[:,0], predictions.Prob_pos)\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "plt.plot(pr, rec, lw=3, label='AUPR = {0:.2f}'.format(aupr))\n",
    "#plt.plot([0, 1], [0, 1], color='gray', lw=3, linestyle='--')\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Precision-Recall Curve Simulated Dataset (38 Cliques of Size 5)')\n",
    "plt.legend()\n",
    "fig.savefig(model_dir + 'prec_recall.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate New Model GCN Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_name:  beta1_power\n",
      "0.31381\n",
      "tensor_name:  beta2_power\n",
      "0.989055\n",
      "tensor_name:  embedding_0\n",
      "[[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.00027301  0.          0.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "tensor_name:  mygcn/mygraphconvolution_1_vars/weights_0\n",
      "[[-0.16212493 -0.24938951 -0.32727623 -0.42024824  0.30041566 -0.02815705\n",
      "  -0.09890167  0.09078765 -0.19150847 -0.0678063  -0.18323191 -0.38914165\n",
      "  -0.1161084  -0.15121168 -0.06650473 -0.06735332 -0.18620574  0.00925534\n",
      "   0.02264159 -0.19767733]\n",
      " [-0.15729332 -0.11331576 -0.30708703 -0.02547364  0.01977869 -0.00594861\n",
      "   0.33748987 -0.16508362 -0.24194832 -0.13410926 -0.05203599 -0.4735086\n",
      "  -0.21282652 -0.16639169  0.04345829  0.02500284 -0.20215476 -0.39018583\n",
      "  -0.16977549 -0.14097953]\n",
      " [-0.10319769 -0.25126001  0.31544903 -0.14658888 -0.11835073 -0.09549821\n",
      "   0.560076    0.27265722 -0.17896606  0.12722002 -0.10813926 -0.44497865\n",
      "  -0.28758439 -0.11216517 -0.16197534 -0.10164028 -0.16244777  0.09164736\n",
      "  -0.14427423 -0.28341126]\n",
      " [-0.09246169 -0.04549635 -0.00698764 -0.03574825 -0.10380521 -0.13062367\n",
      "   0.37313184 -0.09626892 -0.28900146 -0.00487285 -0.10943083 -0.01842203\n",
      "  -0.02050105 -0.15028308 -0.16950662  0.00268908 -0.21665536 -0.14404568\n",
      "  -0.0739468  -0.19641522]\n",
      " [-0.14001743 -0.11002335 -0.20533143 -0.191219   -0.00778329  0.07297204\n",
      "   0.05762015 -0.26746607 -0.27742559 -0.04405937  0.01522285 -0.20235695\n",
      "  -0.14037555 -0.00991452  0.01000975 -0.14349742 -0.16540866  0.27005428\n",
      "  -0.16286039 -0.36692864]\n",
      " [-0.19452025 -0.09114435 -0.22530341 -0.09708114  0.11911906 -0.02678805\n",
      "  -0.01366871 -0.13858967 -0.16632937 -0.0244383  -0.202053   -0.38164279\n",
      "  -0.01101174 -0.13558502 -0.01818541 -0.17228532 -0.18168806  0.00037828\n",
      "  -0.16092983 -0.42872772]\n",
      " [ 0.04318464  0.29919353  0.02832061 -0.36399046 -0.08453926 -0.19523004\n",
      "  -0.03322319 -0.13662712 -0.16639994 -0.04314865  0.03745016 -0.3174797\n",
      "  -0.07893406 -0.05904719  0.08974041 -0.09273954 -0.19748563 -0.24527133\n",
      "  -0.06013411 -0.32063726]\n",
      " [-0.1394338  -0.43909517 -0.06765579 -0.23128575  0.16827963 -0.33428913\n",
      "  -0.11989376 -0.15520197 -0.25479299  0.00277868 -0.07322809 -0.14387459\n",
      "  -0.17265555 -0.15947981 -0.03328366 -0.04491701 -0.18517408 -0.34176224\n",
      "   0.02548679 -0.25596234]\n",
      " [-0.19637401 -0.29912102  0.01168567 -0.16233785  0.35788742 -0.25147352\n",
      "   0.24652053 -0.12032699 -0.24225533  0.06631689 -0.11669399 -0.21701244\n",
      "   0.10145656 -0.15976512 -0.11048111  0.0735751  -0.20836118 -0.12890266\n",
      "  -0.09711847 -0.43809229]\n",
      " [-0.15989055 -0.09294119 -0.22453351 -0.41019264  0.13859868 -0.01449955\n",
      "   0.26012835 -0.1729348  -0.17298242 -0.1036735   0.01147665 -0.49128759\n",
      "  -0.24347535 -0.1320533  -0.16215771 -0.1506432  -0.20320207  0.09640183\n",
      "  -0.11099937 -0.16440292]\n",
      " [-0.21859547 -0.18713048 -0.20316254 -0.35445252  0.29610434 -0.09124205\n",
      "   0.00864773 -0.16731338 -0.28965536  0.05503477  0.06277371 -0.12032644\n",
      "   0.14791265 -0.100857   -0.08496857 -0.01558581 -0.20809294  0.10563561\n",
      "  -0.15931945 -0.11238633]\n",
      " [ 0.06989149  0.01140696 -0.00164395 -0.50183302  0.31271681 -0.24086052\n",
      "  -0.20221974  0.23229402 -0.1490016   0.07595997 -0.19759154 -0.26167738\n",
      "   0.19897468 -0.16836265 -0.0892373  -0.09758946 -0.16709779  0.27881634\n",
      "  -0.15054399  0.02568761]\n",
      " [-0.14248499 -0.02436376 -0.35890603 -0.48900294  0.28390759 -0.08879197\n",
      "  -0.22364749  0.11154163 -0.23048244 -0.13424863 -0.06029782 -0.06786942\n",
      "  -0.20781477 -0.13902664  0.08032725 -0.18641946 -0.20656237  0.13906202\n",
      "   0.06108253 -0.26172265]\n",
      " [-0.15905696 -0.12606157 -0.13828522 -0.33184391  0.18520111 -0.03555457\n",
      "  -0.17908588 -0.06420824 -0.22262721 -0.0203232  -0.19063637 -0.24761926\n",
      "  -0.24798362 -0.05254688 -0.00988244 -0.21047255 -0.14805257 -0.09584327\n",
      "  -0.04414101 -0.44752744]\n",
      " [-0.31686711  0.25186664 -0.11153646 -0.51829356  0.06030202  0.18092783\n",
      "  -0.25250533  0.00088689 -0.16327474 -0.0181904  -0.18405461 -0.55900586\n",
      "  -0.26173908 -0.09977998 -0.14370951 -0.07181964 -0.2097635   0.16560972\n",
      "  -0.15540774 -0.25522119]\n",
      " [-0.04116942 -0.1826549  -0.11983292 -0.45263651  0.31966642 -0.13715132\n",
      "  -0.21242666 -0.21438895 -0.20995824  0.06748757  0.02078071 -0.13524371\n",
      "   0.02584059 -0.18112415  0.02660306 -0.11155653 -0.21381654  0.60255522\n",
      "   0.06746097 -0.18556622]\n",
      " [-0.13609649  0.23995626 -0.26775235 -0.46937498  0.23673894 -0.07120135\n",
      "  -0.12207114 -0.15246704 -0.18814655 -0.01700725 -0.17105268 -0.2012955\n",
      "  -0.05192023 -0.02943759 -0.0387971  -0.09320457 -0.19745882 -0.06942846\n",
      "  -0.06735446 -0.49836841]\n",
      " [-0.12800288 -0.24836719 -0.1530228  -0.1564876  -0.14669394 -0.17400552\n",
      "   0.36202934 -0.11414127 -0.22794536 -0.14203712 -0.12697543 -0.4273552\n",
      "  -0.22174883 -0.02866361 -0.12609074 -0.1953256  -0.1857567  -0.12695606\n",
      "  -0.14967895 -0.26379499]\n",
      " [-0.12762757 -0.24732141  0.35864297  0.13305916 -0.10708456  0.02762973\n",
      "   0.66027939 -0.26830649 -0.17276469 -0.20551278 -0.08315143 -0.13631022\n",
      "  -0.03976275  0.05332619 -0.21638367 -0.16468306 -0.22650978 -0.23042679\n",
      "  -0.15899864 -0.00976668]\n",
      " [-0.23906711 -0.37370643 -0.44365901 -0.30667707 -0.09075719 -0.01929115\n",
      "   0.45654947 -0.16280349 -0.34037355 -0.18138418 -0.04424442 -0.27391288\n",
      "  -0.15559874 -0.1821163  -0.19695133  0.05730766 -0.44288868 -0.27348995\n",
      "   0.23978549 -0.14269614]\n",
      " [ 0.03251165 -0.36055964 -0.11720188  0.13026863 -0.11876692 -0.19748068\n",
      "   0.4170126  -0.1006489  -0.24328919  0.02767726 -0.07542481 -0.21724862\n",
      "   0.17637075 -0.19072348 -0.16846409 -0.09462095 -0.215919    0.03147038\n",
      "   0.02031877  0.00373848]\n",
      " [ 0.03931693  0.00990457 -0.0059352  -0.3209357   0.12274773  0.04465397\n",
      "   0.14113799 -0.09282741 -0.26171905 -0.10703671 -0.09226371 -0.0796275\n",
      "   0.0269821  -0.15323694 -0.1788173  -0.16697432 -0.18440467 -0.11596688\n",
      "   0.05162869 -0.44072235]\n",
      " [-0.25358835 -0.0456885  -0.03496206 -0.16976324 -0.03635335  0.18379004\n",
      "  -0.0235175  -0.19599757 -0.25967321 -0.11865591  0.02511437 -0.32346079\n",
      "  -0.09039117 -0.24998356 -0.15049486  0.10293251 -0.22153749  0.13450266\n",
      "  -0.17317919 -0.02346627]\n",
      " [-0.11277142 -0.20671518  0.28762242 -0.2029976   0.19354989 -0.28686273\n",
      "  -0.25225753  0.07316124 -0.16095698  0.06034437 -0.07077477 -0.17500739\n",
      "   0.10460314 -0.23826157 -0.23686807 -0.23413379  0.01809935 -0.00368191\n",
      "  -0.10775756  0.0462622 ]]\n",
      "tensor_name:  mygcn/mygraphconvolution_1_vars/weights_0/Adam\n",
      "[[ 0.00000884 -0.00000458 -0.00005722  0.00002752  0.00019626  0.00017172\n",
      "  -0.000036    0.00018815 -0.00004862 -0.00001124 -0.00000662  0.00011953\n",
      "   0.00021454 -0.00003282 -0.00005414 -0.00001755 -0.00001366  0.00023611\n",
      "  -0.00004818  0.00017386]\n",
      " [-0.00001579 -0.00001695  0.00003971  0.00018281  0.00028384  0.00023152\n",
      "  -0.00027003  0.00023769  0.00001253 -0.00002404  0.00001394  0.00018648\n",
      "   0.00018134  0.00000023 -0.00003966  0.00001466 -0.00000544  0.00052064\n",
      "  -0.00002066  0.00027198]\n",
      " [-0.0000073   0.0000091  -0.00012077  0.00006983  0.00043652  0.00007182\n",
      "  -0.00047708  0.00018867  0.00000615 -0.00000046  0.00004507  0.00006487\n",
      "   0.00026188 -0.00005287  0.00001151 -0.00001997 -0.00002811  0.0005216\n",
      "  -0.00001836  0.00009534]\n",
      " [ 0.00004909  0.00006272  0.00004615  0.00020011  0.00042912  0.00013887\n",
      "  -0.00047115  0.00020484 -0.00011006  0.00001467  0.00005439  0.00028142\n",
      "   0.00009184  0.0000013  -0.00002681 -0.00001088 -0.00003003  0.00018419\n",
      "  -0.00002894  0.00033141]\n",
      " [-0.00004617 -0.0000272  -0.0000235   0.00007106  0.00060067  0.00006404\n",
      "  -0.00024828  0.00003858 -0.00003121  0.00001757  0.00001848  0.00015259\n",
      "   0.00006907 -0.00002216 -0.00003943 -0.00001085 -0.0000492   0.00023221\n",
      "   0.000006    0.00014698]\n",
      " [ 0.00001001 -0.00004135 -0.00000024  0.00019768  0.00030609 -0.00002906\n",
      "  -0.00005453  0.00026893 -0.00004326 -0.00003124 -0.00000261  0.00011418\n",
      "   0.00006694 -0.00002042 -0.00003431 -0.00000908 -0.00005957  0.00014851\n",
      "  -0.00000418  0.00020363]\n",
      " [-0.00003327  0.00007883  0.00000006  0.00002344  0.0003301   0.00009765\n",
      "  -0.00004112  0.00014916  0.00004562 -0.00004593  0.0000012   0.00010836\n",
      "   0.00023084 -0.00001258 -0.00004408  0.00002623 -0.00004866  0.0002704\n",
      "  -0.00004153  0.00014906]\n",
      " [ 0.00001169 -0.00009209 -0.00002434  0.00004638  0.00041599  0.00006789\n",
      "  -0.00044707  0.00004808 -0.00002019 -0.00000432 -0.00001567  0.00018216\n",
      "   0.00033756  0.00001197 -0.00004162  0.00003088  0.00002381  0.00022338\n",
      "  -0.00004433  0.0002375 ]\n",
      " [ 0.00001664 -0.00002018 -0.00004869  0.00011782  0.00002092  0.00006559\n",
      "   0.00022603  0.00002269 -0.00000261  0.00000761  0.00002044  0.00014131\n",
      "   0.00020735  0.00002645 -0.00002162  0.00001324  0.00001555  0.00007212\n",
      "  -0.00001404  0.00011412]\n",
      " [-0.00001968  0.00002357 -0.00001014  0.0000234   0.00009675  0.00013473\n",
      "  -0.00007248  0.0000134   0.00003398 -0.00001433  0.00000246  0.00006667\n",
      "   0.00016055 -0.00004     0.00001192  0.00000232  0.00000642  0.000071\n",
      "  -0.00006191  0.00024364]\n",
      " [-0.00002581 -0.00002    -0.00004099 -0.00001535  0.00002558  0.00011098\n",
      "   0.00016527 -0.00001347 -0.00004807  0.00001327  0.00003012  0.00017582\n",
      "   0.00015769 -0.0000378  -0.00002962  0.00001019  0.00002164 -0.00000477\n",
      "  -0.00001859  0.00028036]\n",
      " [-0.00001319  0.00004776 -0.00002124  0.00000351  0.0001311   0.00002986\n",
      "   0.00020149  0.0001392   0.00004758 -0.00001267 -0.00000443  0.00013884\n",
      "   0.00011583 -0.00001642 -0.00003902 -0.0000076  -0.00002435  0.00013812\n",
      "  -0.00001626  0.0003549 ]\n",
      " [ 0.00000817  0.00005281 -0.00007714  0.00002748  0.0002586   0.00006736\n",
      "   0.00008974  0.00015171  0.00002838 -0.00001771 -0.00001335  0.00017664\n",
      "   0.00017686  0.0000256  -0.00003052 -0.00001058 -0.0000337   0.00013693\n",
      "  -0.00003361  0.00028194]\n",
      " [ 0.00001898 -0.00003955 -0.00000246  0.00005621  0.00029685  0.00015419\n",
      "  -0.00025518  0.00019048 -0.00006314 -0.00000672 -0.00000228  0.00011943\n",
      "   0.00017437 -0.00002685 -0.00003863 -0.00000557 -0.00007127  0.00034817\n",
      "  -0.00003685  0.00019449]\n",
      " [-0.00003904  0.00009414  0.00010727  0.00009095  0.00007859  0.00016964\n",
      "   0.00028508  0.00014802  0.00004812  0.0000026   0.00000161  0.00006201\n",
      "   0.00011359 -0.00002932 -0.00002488 -0.0000124  -0.00002436  0.00021651\n",
      "   0.00000039  0.00035427]\n",
      " [-0.00003401 -0.00001296 -0.00001835  0.00001315  0.00025454  0.00000399\n",
      "  -0.000055   -0.00000546  0.0000118  -0.00003336  0.00001703  0.00019082\n",
      "   0.00009744 -0.00003778 -0.00003584  0.00001555 -0.00001084  0.00024279\n",
      "  -0.00004093  0.00035175]\n",
      " [-0.00001958  0.00014095  0.00006126  0.00005223  0.00009594  0.00005833\n",
      "   0.00019973  0.00006042  0.00003166 -0.00000449 -0.00000438  0.00017655\n",
      "   0.00009055 -0.00004609 -0.00001771 -0.00003624  0.00002058  0.00007975\n",
      "  -0.00001971  0.00018138]\n",
      " [-0.0000245   0.00001755 -0.00001854  0.00004721  0.00039789  0.00011335\n",
      "  -0.00031451  0.00023     0.00001118 -0.00001577  0.00001978  0.00005387\n",
      "   0.0001279  -0.00003518 -0.00002254 -0.00000762 -0.00007633  0.00024466\n",
      "  -0.0000054   0.00027639]\n",
      " [-0.00004702  0.00010678 -0.00008324  0.00018188  0.00079001  0.00024298\n",
      "  -0.0003566   0.00014232  0.00004734  0.00000544  0.00002558  0.00019755\n",
      "   0.0002346  -0.00000453 -0.00007381  0.00001319 -0.00003407  0.00033661\n",
      "  -0.00001365  0.000374  ]\n",
      " [-0.0000215   0.00011081 -0.00000844  0.00020379  0.00053354  0.00005816\n",
      "  -0.00039906  0.0002093  -0.00007036  0.00005599  0.00001919  0.00038084\n",
      "   0.0006386   0.0000504  -0.0000351   0.00009182 -0.00006475  0.00015094\n",
      "   0.00000269  0.00038023]\n",
      " [-0.00000248  0.0000946  -0.00008243  0.00034008  0.00107614  0.00016597\n",
      "  -0.00086026  0.00024543 -0.00000382 -0.00006249  0.00000993  0.0002967\n",
      "   0.0000979  -0.00002966 -0.000056    0.00002959  0.00003401  0.00041981\n",
      "  -0.00002486  0.0007698 ]\n",
      " [-0.0000213   0.00013066  0.00006716  0.00006553  0.00032067  0.00015271\n",
      "  -0.00027863  0.00021877 -0.00008941 -0.00001985 -0.00001608  0.00021576\n",
      "   0.00033997 -0.00002265 -0.00001068 -0.00001276 -0.00000222  0.0001891\n",
      "  -0.00003524  0.00026608]\n",
      " [ 0.00000318  0.00003267  0.00013858  0.00014717  0.0008035   0.00009658\n",
      "  -0.0007984   0.00023082 -0.00002787  0.00000702  0.00001438  0.00022702\n",
      "   0.00065433 -0.00001717  0.00003863 -0.00002103 -0.00003077  0.00050157\n",
      "   0.00000318  0.00048297]\n",
      " [ 0.00009276 -0.00003814 -0.00007556  0.00026174  0.00093926  0.00003439\n",
      "  -0.00065111  0.00030433  0.00005242 -0.00001788 -0.00000099  0.00041496\n",
      "   0.000317   -0.00000959  0.00000115  0.00001227 -0.00005421  0.00065791\n",
      "  -0.00006408  0.00034813]]\n",
      "tensor_name:  mygcn/mygraphconvolution_1_vars/weights_0/Adam_1\n",
      "[[ 0.          0.          0.          0.          0.00000003  0.00000001\n",
      "   0.00000004  0.          0.          0.          0.          0.00000001\n",
      "   0.00000001  0.          0.          0.          0.          0.          0.\n",
      "   0.00000001]\n",
      " [ 0.          0.          0.          0.00000001  0.00000004  0.00000002\n",
      "   0.00000005  0.          0.          0.          0.          0.00000003\n",
      "   0.          0.          0.          0.          0.          0.00000001\n",
      "   0.          0.00000002]\n",
      " [ 0.          0.          0.          0.          0.00000005  0.00000001\n",
      "   0.00000003  0.          0.          0.          0.          0.00000001\n",
      "   0.00000001  0.          0.          0.          0.          0.00000002\n",
      "   0.          0.00000001]\n",
      " [ 0.          0.          0.          0.          0.00000003  0.00000001\n",
      "   0.00000006  0.          0.          0.          0.          0.00000002\n",
      "   0.          0.          0.          0.          0.          0.00000001\n",
      "   0.          0.00000002]\n",
      " [ 0.          0.          0.          0.          0.00000004  0.00000001\n",
      "   0.00000002  0.          0.          0.          0.          0.00000001\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.00000001]\n",
      " [ 0.          0.          0.          0.          0.00000003  0.00000001\n",
      "   0.00000005  0.          0.          0.          0.          0.00000001\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.00000001]\n",
      " [ 0.          0.          0.          0.          0.00000002  0.00000001\n",
      "   0.00000004  0.          0.          0.          0.          0.00000001\n",
      "   0.00000001  0.          0.          0.          0.          0.00000001\n",
      "   0.          0.00000001]\n",
      " [ 0.          0.          0.          0.          0.00000002  0.\n",
      "   0.00000003  0.          0.          0.          0.          0.00000001\n",
      "   0.00000001  0.          0.          0.          0.          0.          0.\n",
      "   0.00000001]\n",
      " [ 0.          0.          0.          0.          0.00000001  0.00000001\n",
      "   0.00000003  0.          0.          0.          0.          0.00000001\n",
      "   0.00000001  0.          0.          0.          0.          0.          0.\n",
      "   0.00000001]\n",
      " [ 0.          0.          0.          0.          0.00000001  0.00000001\n",
      "   0.00000003  0.          0.          0.          0.          0.00000001\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.00000001]\n",
      " [ 0.          0.          0.          0.          0.00000002  0.00000001\n",
      "   0.00000003  0.          0.          0.          0.          0.00000001\n",
      "   0.00000001  0.          0.          0.          0.          0.          0.\n",
      "   0.00000001]\n",
      " [ 0.          0.          0.          0.          0.00000002  0.\n",
      "   0.00000004  0.          0.          0.          0.          0.00000001\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.00000001]\n",
      " [ 0.          0.          0.          0.          0.00000003  0.00000001\n",
      "   0.00000005  0.          0.          0.          0.          0.00000001\n",
      "   0.00000001  0.          0.          0.          0.          0.          0.\n",
      "   0.00000002]\n",
      " [ 0.          0.          0.          0.          0.00000002  0.00000001\n",
      "   0.00000004  0.          0.          0.          0.          0.00000001\n",
      "   0.          0.          0.          0.          0.          0.00000001\n",
      "   0.          0.00000001]\n",
      " [ 0.          0.          0.00000001  0.          0.00000003  0.00000001\n",
      "   0.00000004  0.          0.          0.          0.          0.00000001\n",
      "   0.          0.          0.          0.          0.          0.00000001\n",
      "   0.          0.00000002]\n",
      " [ 0.          0.          0.          0.          0.00000005  0.00000001\n",
      "   0.00000005  0.          0.          0.          0.          0.00000001\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.00000002]\n",
      " [ 0.          0.          0.          0.          0.00000002  0.00000001\n",
      "   0.00000003  0.          0.          0.          0.          0.00000001\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.00000001]\n",
      " [ 0.          0.          0.          0.          0.00000002  0.00000001\n",
      "   0.00000004  0.          0.          0.          0.          0.00000001\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.00000001]\n",
      " [ 0.          0.00000001  0.          0.00000001  0.00000007  0.00000002\n",
      "   0.00000011  0.00000001  0.          0.          0.          0.00000002\n",
      "   0.00000002  0.          0.          0.          0.          0.00000001\n",
      "   0.          0.00000003]\n",
      " [ 0.          0.00000002  0.00000001  0.00000001  0.00000005  0.00000002\n",
      "   0.0000001   0.00000001  0.          0.          0.          0.00000005\n",
      "   0.00000003  0.          0.          0.          0.          0.00000001\n",
      "   0.          0.00000005]\n",
      " [ 0.          0.00000001  0.          0.00000002  0.00000011  0.00000003\n",
      "   0.00000018  0.00000001  0.          0.          0.          0.00000003\n",
      "   0.00000001  0.          0.          0.          0.          0.00000002\n",
      "   0.          0.00000009]\n",
      " [ 0.          0.          0.          0.          0.00000003  0.00000001\n",
      "   0.00000006  0.          0.          0.          0.          0.00000001\n",
      "   0.00000001  0.          0.          0.          0.          0.00000001\n",
      "   0.          0.00000002]\n",
      " [ 0.          0.          0.00000001  0.00000001  0.00000007  0.00000002\n",
      "   0.00000013  0.00000001  0.          0.          0.          0.00000003\n",
      "   0.00000004  0.          0.          0.          0.          0.00000002\n",
      "   0.          0.00000003]\n",
      " [ 0.          0.          0.          0.00000001  0.00000012  0.00000002\n",
      "   0.00000012  0.00000001  0.          0.          0.          0.00000005\n",
      "   0.00000002  0.          0.          0.          0.          0.00000004\n",
      "   0.          0.00000004]]\n",
      "tensor_name:  mygcn/mygraphconvolution_2_vars/weights_0\n",
      "[[ 0.69124579 -0.38060397  0.24335638 -0.18943347 -0.24169041  0.09782104\n",
      "  -0.20942992  0.7139098  -0.05128823 -0.70182216  0.79265225  0.13595772\n",
      "  -0.44146898 -0.56376529  0.08864592 -0.32547343 -0.28222844 -0.04195435\n",
      "   0.28902516 -0.44633988  0.27449131 -0.17484158  0.58982462  0.59696132\n",
      "   0.18021409 -0.34011635 -0.03518315 -0.23694637  0.17305934 -0.14069493\n",
      "   0.31567851  0.00979878 -0.54795378 -0.24970299 -0.17354247 -0.3871485\n",
      "  -0.27919984 -0.03878003 -0.12165045 -0.12124335]\n",
      " [ 0.3887468   0.26159006 -0.43965128 -0.21230352 -0.75002176  0.08813684\n",
      "  -0.47634548 -0.20432764 -0.33007064 -0.49140868 -0.29879653  0.16232999\n",
      "   0.01007598 -0.36731222  0.83326399 -0.77205718 -0.13095476  0.01075147\n",
      "   0.00256592  0.46092814  0.72967154 -0.493016   -0.27710733 -0.00111498\n",
      "   0.55590993 -0.44751468 -0.28604984 -0.53799778 -0.59117985 -0.47826514\n",
      "   0.07145699 -0.20305756  0.2562376  -0.21098033 -0.33375397 -0.46597555\n",
      "   0.12444592 -0.65355492  0.50512594  0.73712713]\n",
      " [-0.49973419 -0.02533384  0.0173921   0.01793225 -0.64594829  0.21114731\n",
      "   0.40012819 -0.41949767  0.12630129 -0.61677194  0.25286397  0.17244983\n",
      "  -0.20206642 -0.16399994  0.61902499 -0.42405763 -0.18590277  0.02930436\n",
      "  -0.05961173  0.34329909  0.69643927 -0.30486113  0.14274743 -0.38815516\n",
      "   0.24702568 -0.39412579 -0.279237   -0.630301    0.32656983 -0.0857867\n",
      "   0.4929693  -0.87178731  0.69005311 -0.33005863 -0.29856697 -0.51977527\n",
      "  -0.291614   -0.22750001  0.15158959  0.79351747]\n",
      " [-0.4844662   0.04677344 -0.40596578 -0.2821072   0.07370937 -0.54438275\n",
      "  -0.70884353 -0.62081844  0.23462318 -0.59564126 -0.23086524  0.00491612\n",
      "  -0.03308395  0.0898302  -0.13570563 -0.49274847 -0.17810114 -0.2153624\n",
      "  -0.15979125  0.11986604  0.5291419  -0.32888615 -0.24822459 -0.28408408\n",
      "   0.38659364  0.06476885 -0.69199789 -0.63532686 -0.0284876  -0.45787108\n",
      "   0.45703518 -0.55801761  0.23155987 -0.35422051 -0.43825495 -0.35282239\n",
      "  -0.1313798  -0.36895004 -0.26197717  0.44335175]\n",
      " [-0.51195341  0.12401508 -0.30603155  0.22008687  0.68291426  0.62949365\n",
      "  -0.00365773 -0.41078267 -0.52730531  0.5485276  -0.33313474 -0.13490412\n",
      "   0.13439603  0.45143533 -0.38581437  0.50373805  0.34503835  0.34793574\n",
      "  -0.18429981  0.2311241   0.03596102  0.4786219   0.66666782 -0.36781377\n",
      "   0.60424411  0.28879145 -0.32352358 -0.16523081  0.17914006  0.19904515\n",
      "  -0.53856397 -0.21151114  0.10332137 -0.26348194 -0.10440029  0.10511116\n",
      "   0.05145884  0.46534175  0.33453429 -0.27306008]\n",
      " [-0.10061646  0.03790288 -0.57185328  0.52614403  0.5384661   0.26962361\n",
      "  -0.47326177 -0.43531319 -0.42428172 -0.38833794 -0.59724343 -0.06001119\n",
      "  -0.24623442 -0.22448479 -0.13919374 -0.75721115 -0.12353755 -0.50930238\n",
      "   0.12549968 -0.12727174  0.06751086 -0.54844713  0.27262393 -0.19061132\n",
      "   0.15921968 -0.04660449 -0.61187083 -0.34570652 -0.00137994 -0.17915493\n",
      "   0.12212279  0.08284566  0.28865528 -0.14851703 -0.07356672 -0.26113629\n",
      "   0.32202172 -0.06631637  0.24165341  0.10042811]\n",
      " [-0.2587375  -0.70440662 -0.1325772   0.07246311 -0.68683541 -0.71326298\n",
      "  -0.14475638 -0.68338734  0.23164304 -0.52028126 -0.50672179 -0.12752926\n",
      "   0.15323707  0.31897941 -0.94191259 -0.21705548 -0.51519042 -0.21623425\n",
      "  -0.42572823  0.25589243  0.4620274  -0.47515336 -0.50626433 -0.07929108\n",
      "   0.09574268 -0.05574858 -0.7839542  -0.41675365 -0.64334542 -0.50831926\n",
      "   0.52012259 -0.46612039  0.01040202 -0.00334441 -0.0861184  -0.28174308\n",
      "  -0.2265382  -0.0054585  -0.43056309 -0.26130325]\n",
      " [ 0.13487458 -0.01108168 -0.17215326 -0.32729346 -0.44399855  0.26224819\n",
      "  -0.37480175 -0.59382516 -0.11987688 -0.64031285  0.11698485 -0.27078465\n",
      "  -0.30357498 -0.84719396  0.21504575 -0.64344633 -0.52201527 -0.50761712\n",
      "   0.06870335  0.32361349 -0.03252075 -0.36209121  0.47713116 -0.07369944\n",
      "   0.06830475  0.03476903 -0.24715324 -0.28382289 -0.58666182 -0.40308493\n",
      "   0.02069351  0.30932128 -0.51923585  0.29159474 -0.28500566 -0.10398727\n",
      "   0.04101675 -0.451588    0.22369601  0.18359083]\n",
      " [ 0.66621137 -0.38249299 -0.05936667 -0.22380593 -0.41882065  0.65292108\n",
      "  -0.30010578 -0.38030785 -0.44433653 -0.30485919 -0.1502391   0.20270188\n",
      "  -0.39745978 -0.1805345   0.28358775 -0.44041765 -0.54990083 -0.29665604\n",
      "   0.24674596  0.39226025  0.28430438  0.78440273  0.61230928  0.39605135\n",
      "   0.4760524  -0.28231806  0.01843539 -0.01996076 -0.29205713 -0.15399851\n",
      "   0.15494001 -0.51822537 -0.00238153  0.34369063 -0.30963218  0.03529099\n",
      "  -0.21400508 -0.23734936  0.43716922 -0.40095386]\n",
      " [-0.06158955  0.18779756  0.26938501  0.17400517 -0.37302291  0.75782228\n",
      "  -0.03957063 -0.34187537 -0.56878936  0.33522061 -0.01284953  0.41975155\n",
      "   0.47805661  0.28199506 -0.38052562  0.47339308  0.25898984 -0.76969564\n",
      "  -0.0777024   0.06265708 -0.21013504  0.20550826  0.42891094 -0.19153333\n",
      "   0.35070398 -0.10436635 -0.55122507 -0.28941268  0.09731923 -0.47855327\n",
      "   0.26123878 -0.375267   -0.57326329  0.04962573 -0.16746604 -0.51050454\n",
      "   0.12547362 -0.40256995  0.67732888  0.15361638]\n",
      " [ 0.42016706 -0.20492049 -0.07988494  0.04470049  0.69241786  0.30125266\n",
      "   0.13384256 -0.25258228 -0.65232629 -0.51915014 -0.60063249  0.24953018\n",
      "   0.49125335  0.13791659  0.03904809 -0.14862181 -0.32528397 -0.35508206\n",
      "   0.01025209  0.22417827  0.36564094  0.01585631  0.52863908  0.31138849\n",
      "  -0.28013387 -0.339816   -0.28348893 -0.40710726  0.23561448 -0.45500454\n",
      "   0.24883312 -0.40779048 -0.19217461 -0.17993368  0.20330915 -0.19803095\n",
      "   0.14271393  0.40197641  0.18917999 -0.57421172]\n",
      " [-0.37155467 -0.28620833  0.12563054 -0.24897309  0.11185309  0.21124732\n",
      "  -0.6267488  -0.31338105  0.26142618  0.36883318 -0.38800314  0.3013559\n",
      "  -0.47359326 -0.13336158  0.23837984 -0.5576387  -0.58906138 -0.50732905\n",
      "  -0.01628766  0.25493777  0.49069536 -0.83985412 -0.14943066  0.22162507\n",
      "   0.07530273 -0.03206722 -0.55612737 -0.54268813 -0.33647254 -0.55709255\n",
      "   0.09831112 -0.22273029  0.27734581 -0.50771427 -0.04860908 -0.34558499\n",
      "  -0.24626213 -0.19504519  0.37544087  0.07885158]\n",
      " [ 0.80560112 -0.61885381  0.57280231  0.02813819  0.15097684  0.15150663\n",
      "   0.18720645  0.27896574 -0.24313366 -0.5018304  -0.19235861 -0.30144215\n",
      "  -0.12584324 -0.63918477 -0.42663011  0.31850532  0.28045771 -0.19467247\n",
      "  -0.25378355 -0.3334136   0.11271866  0.24851105 -0.31115016  0.06496129\n",
      "   0.28957644 -0.33139965 -0.01272475 -0.52584279 -0.15392943 -0.30427703\n",
      "  -0.42497146  0.84623224  0.1851348  -0.07878305 -0.2796194   0.09220368\n",
      "  -0.09885793  0.07610916  0.36162356 -0.38779652]\n",
      " [-0.79695904  0.49418327  0.16464731 -0.04100212  0.37764058  0.6173721\n",
      "   0.2845442  -0.45009562  0.09492507  0.06537025 -0.80408955  0.47790492\n",
      "  -0.60258108 -0.03696375  0.13723364 -0.47442877  0.2956644  -0.3225137\n",
      "   0.61489475 -0.12692405 -0.61449909 -0.22934031  0.07244089  0.14063337\n",
      "   0.45967045 -0.51329738 -0.29204535  0.05650651  0.11190501 -0.6277554\n",
      "   0.03174644  0.4241077  -0.55420685 -0.45705277 -0.1646962  -0.47536793\n",
      "  -0.29634795  0.30160704  0.36942184 -0.39017013]\n",
      " [ 0.63558161  0.40088534  0.36533493  0.25922766 -0.1342704   0.41693369\n",
      "  -0.60752428  0.36704928 -0.10879971 -0.4676002  -0.34008592  0.19881901\n",
      "   0.37794021 -0.51970774  0.12620071  0.21313486 -0.20948601 -0.45963022\n",
      "  -0.00789508  0.15725431 -0.37751189  0.28182682  0.1958358   0.52902532\n",
      "   0.39434037  0.02949168  0.54173613 -0.41277304 -0.44390219  0.30343083\n",
      "  -0.21808812  0.65329671 -0.30342624  0.14626037  0.21707746 -0.11129201\n",
      "  -0.17526373 -0.29068261  0.20237851 -0.33599848]\n",
      " [ 0.51731187 -0.24276794  0.30656636 -0.38496923 -0.57877421  0.16305752\n",
      "   0.01642956 -0.20832597 -0.3102037  -0.51135898  0.29256713 -0.0042806\n",
      "  -0.62017202 -0.46252924  0.33689851  0.73647279  0.15978771  0.198588\n",
      "   0.01537704 -0.30354619 -0.19770908  0.10093684  0.17316818  0.62287027\n",
      "   0.6203171  -0.65464675  0.22122666 -0.60585111  0.51004666  0.42022419\n",
      "   0.25150231  0.47540623 -0.53113472  0.30787024  0.05129763 -0.5550496\n",
      "  -0.11735798  0.51370597  0.691742   -0.13643456]\n",
      " [-0.63129193 -0.21886683 -0.7627492   0.08272633 -0.45027179 -0.18533474\n",
      "  -0.52606738 -0.15486051  0.29596218 -0.17850879 -0.5294435   0.49716097\n",
      "  -0.56319523  0.54150701 -0.41309157 -0.23734532 -0.420109    0.2665146\n",
      "  -0.52488166  0.42043734  0.14550187 -0.0992349   0.28140467  0.36913058\n",
      "   0.47184527  0.51677227 -0.58794492 -0.15553057 -0.41675469  0.31174788\n",
      "   0.65469277 -0.16664593  0.16446093 -0.61438769 -0.01418927  0.44395953\n",
      "  -0.23874001  0.6059556   0.35890135 -0.69177771]\n",
      " [ 0.15018667 -0.69315952 -0.62332708  0.205926   -0.549842    0.86197537\n",
      "  -0.43395141  0.5984053  -0.02853317 -0.53587842 -0.11058567 -0.33297387\n",
      "  -0.56600177 -0.39770019 -0.31152654  0.69224972  0.03775403 -0.41570407\n",
      "  -0.22260608 -0.09199592 -0.17111586  0.09602322  0.1401621  -0.40257791\n",
      "  -0.11088741 -0.6031974   0.26619986 -0.16197425 -0.34999067  0.2021648\n",
      "  -0.12787911  0.34595034 -0.3373141  -0.14493616 -0.00466165 -0.68558562\n",
      "  -0.24419121 -0.19349778 -0.10383127  0.14099075]\n",
      " [ 0.24021605  0.4674429  -0.29400271 -0.22717682  0.1504575  -0.6814959\n",
      "  -0.36684    -0.39741415 -0.141123   -0.24275608 -0.48654637 -0.27539942\n",
      "  -0.06700091  0.45022732  0.41773972 -0.54907006 -0.70570761 -0.2284085\n",
      "  -0.4838841   0.12233476 -0.60872024 -0.12840997  0.42806807  0.57437378\n",
      "   0.62305605  0.33991694 -0.16980138 -0.6368767  -0.1177934  -0.46977815\n",
      "   0.314991   -0.13344729 -0.62671572  0.15133183 -0.18986487  0.47447217\n",
      "   0.14785728 -0.20523506 -0.48991713  0.43044257]\n",
      " [-0.21993217 -0.74533731 -0.54859579  0.0828798  -0.2449412  -0.04096231\n",
      "  -0.47405562 -0.41793174 -0.03034435 -0.27708116 -0.56053734 -0.1388423\n",
      "  -0.68158245 -0.1345813  -0.48273137 -0.15641655 -0.46849516 -0.5271616\n",
      "  -0.1538219   0.37857381  0.08432157 -0.03957315  0.22406974 -0.00315492\n",
      "   0.14379087 -0.4350875  -0.32023558 -0.55416656 -0.38722387  0.03891692\n",
      "   0.39002421 -0.11003362  0.18075365  0.07648247 -0.4844715  -0.15085703\n",
      "  -0.14044991 -0.44209754 -0.24825792 -0.10832894]]\n",
      "tensor_name:  mygcn/mygraphconvolution_2_vars/weights_0/Adam\n",
      "[[-0.00000091  0.00000041 -0.00000099  0.          0.00000027 -0.00002555\n",
      "   0.         -0.00000089  0.00002571  0.00000094 -0.00000159 -0.00002778\n",
      "   0.00000299  0.0000224  -0.0000013   0.00000194  0.          0.00001689\n",
      "  -0.00000477  0.00003764 -0.00000324  0.00000046 -0.00002926 -0.0000147\n",
      "  -0.00004082  0.00001858  0.00001066  0.00000021  0.          0.00001911\n",
      "   0.00000082  0.00000006  0.00002336  0.00000082  0.          0.00001083\n",
      "   0.          0.          0.00001975 -0.00000023]\n",
      " [-0.00000039 -0.00000451  0.0000215  -0.00000001  0.00000129  0.00006749\n",
      "   0.00000516  0.0000277   0.00003027  0.00000221  0.00003887  0.00007086\n",
      "   0.00000112  0.00010561 -0.00003291  0.00003877  0.00000148  0.00002222\n",
      "   0.00003249 -0.00008379 -0.00004829  0.00000831  0.00003836  0.00000678\n",
      "  -0.00003397  0.00005482  0.00003271  0.0000019   0.0000001   0.00002437\n",
      "  -0.00001933  0.00001416 -0.00003979  0.00001192  0.00001597  0.00001599\n",
      "   0.          0.00000028 -0.00001137 -0.00001258]\n",
      " [ 0.00000781 -0.00000135  0.00001137  0.00001201 -0.00000046  0.00000287\n",
      "  -0.00000275  0.00000565  0.00008541  0.00001296 -0.00000228  0.00001205\n",
      "   0.00003215  0.00002158 -0.00002849  0.00002436  0.0000035   0.00006181\n",
      "   0.00005033 -0.00005398 -0.00005298  0.00000595  0.00001845  0.00003627\n",
      "   0.00005765  0.00005463  0.00006679  0.00000041 -0.00000001  0.00006116\n",
      "  -0.00006285  0.00000981 -0.00008814  0.00000116  0.00000426  0.00003768\n",
      "   0.         -0.00000033 -0.00001097 -0.00001078]\n",
      " [ 0.00002479  0.00002996  0.00016839  0.         -0.00000012  0.00017328\n",
      "   0.00000055  0.00000545 -0.0000613   0.00000143  0.00001593  0.00049025\n",
      "   0.00003019 -0.0000131   0.00000411  0.00000664  0.          0.0000492\n",
      "   0.0002428  -0.00014627 -0.00010372  0.00003938  0.00033678  0.00019304\n",
      "   0.00015281  0.00003109  0.0000575   0.00000039  0.00000002  0.00006993\n",
      "  -0.0001662   0.00001109 -0.00004365  0.00001425  0.00000326  0.00007302\n",
      "   0.          0.00000043 -0.00000512 -0.00003218]\n",
      " [-0.00000959  0.00005706 -0.00000266  0.00005239  0.00007959  0.00058928\n",
      "   0.0000041  -0.00000018 -0.00005828  0.00026875  0.000002    0.\n",
      "   0.00003764 -0.00030256  0.00001193 -0.0003352  -0.00003493  0.00024708\n",
      "   0.00000009 -0.00095878 -0.00009887  0.00003944  0.00073347  0.00000022\n",
      "   0.00025864  0.00003908 -0.00000066  0.00000019  0.00005066 -0.00006922\n",
      "   0.00046152  0.00005603  0.00010434 -0.00000623  0.         -0.00000464\n",
      "   0.00001769 -0.00005735 -0.00013133 -0.00000514]\n",
      " [ 0.00007238  0.00002281  0.00010846 -0.00001902  0.00000043 -0.00000463\n",
      "   0.00000049  0.00000034  0.00019287  0.00000083  0.00006786  0.00040038\n",
      "   0.0000418   0.0002059  -0.00001415  0.00000931 -0.00000016  0.00008644\n",
      "   0.0001564   0.00022504 -0.0000141   0.00004458 -0.00003659  0.00008928\n",
      "   0.00007916  0.00007926  0.00003555  0.00000016  0.          0.00003497\n",
      "   0.00002682  0.00001231  0.00011945  0.00009911  0.00002244  0.00004762\n",
      "  -0.00000503  0.00000197 -0.00002134 -0.00001445]\n",
      " [ 0.00056737  0.00003673  0.0002037  -0.00013834  0.00005055  0.00049085\n",
      "   0.0000182   0.00007112 -0.00033213  0.00019731  0.00032725  0.00047455\n",
      "  -0.00009251 -0.00022308  0.00006684 -0.00016623  0.00003243  0.0002908\n",
      "   0.00026862 -0.00102919  0.00004744  0.00009687  0.00130542  0.0001478\n",
      "   0.0005854   0.00011873  0.00008684  0.00000753  0.00002548  0.00006277\n",
      "   0.00045914  0.00009081  0.00027385  0.00018786  0.          0.00005453\n",
      "  -0.00000166 -0.00001323  0.00012648 -0.00001973]\n",
      " [ 0.00017406  0.00002042  0.00016726  0.00000645  0.00000837 -0.00010583\n",
      "   0.00000911  0.00001219  0.00019057  0.00002834  0.00022807  0.00048151\n",
      "   0.0000637   0.00054953  0.0000412   0.00000813 -0.00000229  0.00013316\n",
      "   0.0002268  -0.00005053  0.00034247  0.00003474 -0.00014138  0.00010286\n",
      "   0.00034037  0.00013064  0.00008422  0.00000202  0.00000005  0.00006535\n",
      "   0.00003191  0.00003133  0.00051956  0.000067    0.          0.00010339\n",
      "  -0.00000542  0.00000199  0.00005114  0.00002468]\n",
      " [-0.00000449  0.00000785 -0.00000177  0.          0.00000947 -0.00003248\n",
      "   0.00002221  0.00009731  0.00000777  0.00004092  0.00005545 -0.00003004\n",
      "   0.00002531  0.00004789 -0.00001658  0.0000549   0.00000102  0.0000164\n",
      "  -0.00000449 -0.00019193 -0.00000599 -0.00000143 -0.00009228 -0.00002043\n",
      "  -0.00005456  0.0000114   0.0000041  -0.00000001  0.00000067  0.00001877\n",
      "   0.00000043  0.00001097  0.0000281  -0.00001351  0.          0.000023    0.\n",
      "   0.         -0.00000944  0.00001824]\n",
      " [ 0.00000036 -0.0000013  -0.00000066 -0.00000044  0.00000058 -0.00000448\n",
      "   0.          0.00000318  0.0000027   0.00000138  0.00000002 -0.00000166\n",
      "  -0.00000186 -0.00000199  0.00000374 -0.00000278  0.         -0.00000012\n",
      "   0.00000173 -0.00000392 -0.00000196 -0.00000015 -0.00000321 -0.00000013\n",
      "   0.00000114  0.00000137  0.00000769  0.          0.00000001  0.00000291\n",
      "  -0.00000012  0.00000286  0.00000054 -0.0000002   0.          0.00000101\n",
      "   0.          0.00000097 -0.00000112 -0.00000059]\n",
      " [-0.00000107  0.00000007  0.00000005 -0.00000153 -0.00000148 -0.00000699\n",
      "   0.          0.00000352  0.00000057  0.00000008  0.00000271 -0.00000082\n",
      "  -0.00000023 -0.0000075   0.00000009  0.00000375  0.00000011  0.00000111\n",
      "  -0.00000009 -0.00002112 -0.00000332 -0.00000051 -0.00000052 -0.00000112\n",
      "   0.00001294  0.00000089  0.0000007   0.00000064 -0.00000002  0.00000051\n",
      "  -0.00001017  0.00000094 -0.00000195  0.0000004   0.          0.00000096\n",
      "   0.          0.00000077 -0.0000007   0.00000091]\n",
      " [ 0.00000652  0.00000429  0.00017048  0.         -0.00000002  0.00001472\n",
      "   0.00000045  0.0000264   0.00008611 -0.00000351  0.00001364  0.00036336\n",
      "   0.00005741  0.00010885 -0.00001371  0.00001798  0.00000031  0.00010006\n",
      "   0.00013985 -0.00017199 -0.00006273  0.00000778  0.00001717  0.00009674\n",
      "   0.00011663  0.00011417  0.00002024  0.00000131  0.00000004  0.00011744\n",
      "  -0.00012169 -0.00000271  0.00002083  0.00001625  0.          0.00010082\n",
      "   0.          0.0000016  -0.00003698 -0.00001138]\n",
      " [-0.00005377  0.0000036  -0.00000991  0.00002739 -0.00000134  0.00001455\n",
      "   0.00000508 -0.00000312 -0.00001237 -0.00001588  0.00000206  0.\n",
      "  -0.00001284 -0.00005913  0.00000149 -0.0000123  -0.00001569  0.00002786\n",
      "   0.         -0.00012807 -0.00008651 -0.00006826  0.00017409 -0.00000002\n",
      "   0.00015257  0.00000632  0.00000163  0.00000013 -0.00001027 -0.00000614\n",
      "   0.00002933 -0.00006928 -0.00004073 -0.00000258  0.         -0.00000123\n",
      "   0.00000543 -0.00000476  0.00002128  0.0000001 ]\n",
      " [ 0.00000149 -0.00000045 -0.00000272  0.         -0.00000015 -0.0000141\n",
      "   0.          0.00000029  0.0000054  -0.00000004  0.00000051 -0.00000772\n",
      "   0.00000664  0.00001679 -0.00000014  0.00000056  0.          0.00000815\n",
      "  -0.00000448  0.00001657  0.00000708  0.00000083 -0.0000134  -0.00000746\n",
      "  -0.00001709  0.00001493  0.00000717  0.00000001  0.          0.00001035\n",
      "   0.00000634 -0.00000025  0.00000271  0.00000023  0.          0.00001098\n",
      "   0.          0.         -0.00000412  0.00000037]\n",
      " [-0.00000355 -0.00000031 -0.00000005  0.          0.00000302 -0.00001636\n",
      "   0.00000487 -0.0000027   0.00000862  0.00000768  0.00000034 -0.00000114\n",
      "  -0.00000581  0.00000235 -0.00000866 -0.00000261  0.00000028  0.00000081\n",
      "   0.         -0.0000069   0.00000314 -0.00000096 -0.00001415 -0.00000122\n",
      "  -0.00000838  0.00000002 -0.00000497  0.00000123  0.00000001 -0.00000038\n",
      "   0.00000004 -0.0000035   0.00001132 -0.00000332  0.          0.          0.\n",
      "   0.00000031 -0.00000125  0.00000441]\n",
      " [-0.00000468  0.00000046 -0.00000005  0.00000124  0.00000057 -0.00001762\n",
      "  -0.          0.00000067  0.00000706  0.00000396 -0.00000231 -0.00000066\n",
      "   0.00000011  0.00000605 -0.0000039  -0.00000603  0.          0.0000004\n",
      "   0.00000015  0.00000332  0.00000062 -0.00000059 -0.00000833 -0.00000103\n",
      "  -0.00000861  0.00000136 -0.00000728  0.00000004 -0.00000001 -0.00000021\n",
      "  -0.00000101 -0.00000175  0.00000517 -0.00000181  0.          0.00000072\n",
      "   0.         -0.00000024 -0.00000128  0.00000177]\n",
      " [ 0.00000013  0.00000336  0.00000031  0.          0.00000561  0.00004785\n",
      "   0.00000179  0.00008691 -0.00000003  0.00001057  0.00004704 -0.00000942\n",
      "   0.00000702 -0.00000521  0.00000522  0.00006245  0.00000207 -0.00000408\n",
      "   0.00000029 -0.00018136 -0.00000927  0.00000084 -0.00002738 -0.0000021\n",
      "  -0.00000451 -0.00001664  0.00001166  0.00000685  0.00000039 -0.00000143\n",
      "  -0.00001911  0.00002735 -0.00000363  0.00000133  0.         -0.00000018\n",
      "   0.         -0.00000121 -0.00000459  0.00000698]\n",
      " [ 0.00009393  0.00001627  0.0000226   0.00005556  0.00002161 -0.00002137\n",
      "   0.00000267 -0.00002082 -0.00000126  0.00001146  0.00004008  0.00006901\n",
      "   0.00002565 -0.00006299  0.00002421 -0.00016611 -0.00001357  0.00007144\n",
      "   0.00003754 -0.00044121 -0.00019403  0.0000808   0.00041333  0.0000141\n",
      "   0.00045845  0.00000834 -0.00000021  0.00000053 -0.00000722 -0.00001327\n",
      "   0.00020248  0.00006651  0.00011508 -0.00002436  0.          0.0000095\n",
      "   0.00000288 -0.0000197  -0.00005345  0.00001951]\n",
      " [-0.00000258 -0.00000093  0.00000011  0.         -0.00000042  0.00001223\n",
      "   0.00000181  0.00001614  0.00001325  0.0000097   0.00001     0.\n",
      "  -0.00000075 -0.00001044 -0.00000145  0.00001528  0.0000003   0.00000136\n",
      "   0.00000008 -0.00003553  0.00000112  0.00000016 -0.00000781 -0.00000106\n",
      "  -0.00000801 -0.00000295  0.00001518  0.00000078  0.          0.00000008\n",
      "  -0.00000149  0.00000551 -0.00000028 -0.0000013   0.         -0.00000014\n",
      "   0.         -0.00000008  0.00000117 -0.00000001]\n",
      " [ 0.00009028  0.00002243  0.00015851 -0.00000043  0.00001282  0.00018936\n",
      "   0.00002206  0.00005469  0.00035921  0.00002464  0.00012948  0.00022874\n",
      "   0.00009242  0.00050037  0.00007206  0.00006482  0.0000005   0.00003582\n",
      "   0.00011429  0.00022842  0.0000782   0.00005673  0.00009487  0.00008845\n",
      "  -0.00001245  0.00009914  0.00021076  0.00000622  0.00000022  0.00011666\n",
      "   0.00005454  0.00003068  0.00025766  0.00010452  0.0000225   0.00011294\n",
      "   0.          0.00000122  0.0000177   0.00000284]]\n",
      "tensor_name:  mygcn/mygraphconvolution_2_vars/weights_0/Adam_1\n",
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.00000001\n",
      "   0.          0.          0.          0.          0.          0.00000001\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.00000001  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.00000001  0.          0.          0.00000002  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.00000001\n",
      "   0.          0.          0.          0.          0.00000002  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.00000001  0.          0.          0.00000002\n",
      "   0.          0.          0.00000007  0.          0.          0.00000019\n",
      "   0.          0.00000002  0.          0.          0.          0.\n",
      "   0.00000005  0.00000023  0.00000001  0.          0.00000008  0.00000003\n",
      "   0.00000017  0.          0.          0.          0.          0.\n",
      "   0.00000003  0.          0.00000005  0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.00000004\n",
      "   0.          0.          0.          0.00000002  0.          0.          0.\n",
      "   0.00000001  0.          0.00000001  0.00000001  0.          0.\n",
      "   0.00000006  0.          0.00000001  0.00000007  0.          0.00000001\n",
      "   0.          0.          0.          0.          0.          0.00000001\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.00000003  0.          0.          0.00000013  0.\n",
      "   0.00000001  0.          0.          0.          0.          0.00000002\n",
      "   0.00000013  0.00000001  0.          0.00000002  0.00000001  0.00000007\n",
      "   0.          0.          0.          0.          0.          0.00000001\n",
      "   0.          0.00000001  0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.00000008  0.          0.00000001  0.          0.          0.00000004\n",
      "   0.          0.          0.00000023  0.          0.00000001  0.00000039\n",
      "   0.00000001  0.00000014  0.          0.00000001  0.          0.\n",
      "   0.00000006  0.00000061  0.00000018  0.00000002  0.00000019  0.00000004\n",
      "   0.00000044  0.00000001  0.          0.          0.          0.\n",
      "   0.00000026  0.00000001  0.00000012  0.00000002  0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.00000001  0.          0.00000001  0.          0.          0.00000001\n",
      "   0.          0.          0.00000008  0.          0.00000001  0.00000023\n",
      "   0.          0.00000004  0.00000001  0.          0.          0.00000001\n",
      "   0.00000006  0.0000003   0.00000004  0.          0.00000003  0.00000001\n",
      "   0.00000023  0.00000001  0.          0.          0.          0.\n",
      "   0.00000003  0.          0.00000006  0.          0.          0.          0.\n",
      "   0.          0.00000001  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.00000001  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.00000004\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.00000001  0.          0.          0.          0.\n",
      "   0.          0.00000009  0.          0.          0.00000016  0.\n",
      "   0.00000004  0.          0.          0.          0.00000001  0.00000002\n",
      "   0.00000009  0.00000001  0.          0.00000003  0.00000002  0.00000007\n",
      "   0.00000001  0.          0.          0.          0.00000001  0.00000001\n",
      "   0.          0.00000002  0.          0.          0.00000001  0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.00000002\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.00000001\n",
      "   0.          0.          0.00000001  0.          0.          0.00000001\n",
      "   0.          0.00000001  0.          0.          0.          0.          0.\n",
      "   0.00000002  0.00000002  0.          0.00000002  0.          0.00000002\n",
      "   0.          0.          0.          0.          0.          0.00000001\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.00000001  0.          0.          0.00000004\n",
      "   0.          0.          0.00000011  0.          0.          0.00000019\n",
      "   0.          0.00000005  0.          0.          0.          0.\n",
      "   0.00000003  0.00000028  0.00000002  0.          0.00000004  0.00000002\n",
      "   0.00000012  0.00000001  0.00000001  0.          0.          0.00000001\n",
      "   0.00000002  0.          0.00000006  0.          0.          0.00000001\n",
      "   0.          0.          0.          0.        ]]\n",
      "tensor_name:  mygcn/mygraphconvolution_3_vars/weights_0\n",
      "[[ 0.39158189 -0.00416729]\n",
      " [ 0.22732839 -0.20410249]\n",
      " [ 0.2613048  -0.0401573 ]\n",
      " [-0.31635886  0.25347611]\n",
      " [-0.08461331  0.02756722]\n",
      " [-0.36379746  0.49073905]\n",
      " [ 0.07190463 -0.02163227]\n",
      " [ 0.1636622  -0.17595702]\n",
      " [ 0.27497947 -0.20140931]\n",
      " [-0.36321792  0.23036137]\n",
      " [ 0.01264622 -0.03699712]\n",
      " [-0.28449583  0.14379577]\n",
      " [ 0.40402904 -0.06314927]\n",
      " [ 0.3360976  -0.25808027]\n",
      " [-0.51575708  0.39442891]\n",
      " [ 0.23687649 -0.24994993]\n",
      " [-0.61480701  0.02762723]\n",
      " [-0.03299119  0.399524  ]\n",
      " [ 0.14899512 -0.00972223]\n",
      " [ 0.42287952 -0.35410258]\n",
      " [ 0.35166517 -0.10965314]\n",
      " [ 0.3667686  -0.30408093]\n",
      " [-0.60514009  0.38232952]\n",
      " [ 0.00920801  0.00239339]\n",
      " [-0.21017365  0.35102332]\n",
      " [-0.03654703  0.17000942]\n",
      " [ 0.18799946 -0.2124657 ]\n",
      " [ 0.1977843  -0.53299338]\n",
      " [-0.77004713  0.06359679]\n",
      " [ 0.04336221  0.5590905 ]\n",
      " [ 0.25914872 -0.37829939]\n",
      " [ 0.61035413 -0.11974595]\n",
      " [ 0.02412055 -0.35886228]\n",
      " [ 0.00880313  0.49786016]\n",
      " [ 0.00023681  0.03234538]\n",
      " [ 0.1148087   0.36857268]\n",
      " [ 0.07740781 -0.22287588]\n",
      " [ 0.15759365 -0.02776923]\n",
      " [-0.20984828  0.19564724]\n",
      " [-0.09109305  0.31296396]]\n",
      "tensor_name:  mygcn/mygraphconvolution_3_vars/weights_0/Adam\n",
      "[[-0.0000044   0.0000044 ]\n",
      " [-0.00003145  0.00003145]\n",
      " [-0.00016014  0.00016014]\n",
      " [-0.00006858  0.00006858]\n",
      " [-0.00008431  0.00008431]\n",
      " [-0.00032402  0.00032402]\n",
      " [-0.00001368  0.00001368]\n",
      " [-0.00002502  0.00002502]\n",
      " [ 0.00014243 -0.00014243]\n",
      " [-0.00009822  0.00009822]\n",
      " [ 0.00008909 -0.00008909]\n",
      " [-0.00051345  0.00051345]\n",
      " [-0.00011215  0.00011215]\n",
      " [-0.00024506  0.00024506]\n",
      " [ 0.00002598 -0.00002598]\n",
      " [-0.00040391  0.00040391]\n",
      " [ 0.00003286 -0.00003286]\n",
      " [-0.00003001  0.00003001]\n",
      " [-0.00045819  0.00045819]\n",
      " [-0.00004296  0.00004296]\n",
      " [ 0.0001115  -0.0001115 ]\n",
      " [ 0.00000957 -0.00000957]\n",
      " [-0.00012892  0.00012892]\n",
      " [-0.00021381  0.00021381]\n",
      " [-0.00084158  0.00084158]\n",
      " [ 0.00018182 -0.00018182]\n",
      " [-0.00003145  0.00003145]\n",
      " [-0.0000063   0.0000063 ]\n",
      " [-0.00000086  0.00000086]\n",
      " [ 0.00014421 -0.00014421]\n",
      " [-0.00002849  0.00002849]\n",
      " [-0.00006318  0.00006318]\n",
      " [ 0.00021962 -0.00021962]\n",
      " [ 0.00082019 -0.00082019]\n",
      " [ 0.0000011  -0.0000011 ]\n",
      " [ 0.00023567 -0.00023567]\n",
      " [ 0.00000483 -0.00000483]\n",
      " [-0.0002372   0.0002372 ]\n",
      " [ 0.00003057 -0.00003057]\n",
      " [-0.00002507  0.00002507]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_name:  mygcn/mygraphconvolution_3_vars/weights_0/Adam_1\n",
      "[[ 0.00000001  0.00000001]\n",
      " [ 0.          0.        ]\n",
      " [ 0.00000001  0.00000001]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.00000003  0.00000003]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.00000012  0.00000012]\n",
      " [ 0.          0.        ]\n",
      " [ 0.00000001  0.00000001]\n",
      " [ 0.00000017  0.00000017]\n",
      " [ 0.          0.        ]\n",
      " [ 0.00000004  0.00000004]\n",
      " [ 0.          0.        ]\n",
      " [ 0.00000002  0.00000002]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.00000017  0.00000017]\n",
      " [ 0.00000041  0.00000041]\n",
      " [ 0.0000003   0.0000003 ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.00000002  0.00000002]\n",
      " [ 0.00000003  0.00000003]\n",
      " [ 0.00000061  0.00000061]\n",
      " [ 0.00000009  0.00000009]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.00000001  0.00000001]\n",
      " [ 0.00000038  0.00000038]\n",
      " [ 0.          0.        ]\n",
      " [ 0.00000011  0.00000011]\n",
      " [ 0.00000011  0.00000011]\n",
      " [ 0.          0.        ]\n",
      " [ 0.00000007  0.00000007]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "checkpoint_path = os.path.join(model_dir, \"model.ckpt\")\n",
    "print_tensors_in_checkpoint_file(file_name=checkpoint_path, tensor_name='', all_tensors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-Parameters read from ../data/GCN/training/2018_01_22_17_10_15/hyper_params.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'decay': 0.0005,\n",
       " 'dropout': 0.5,\n",
       " 'epochs': 10,\n",
       " 'hidden1': 20,\n",
       " 'hidden2': 40,\n",
       " 'loss_mul': 1,\n",
       " 'lr': 0.1,\n",
       " 'support': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def str_to_num(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        return float(s)\n",
    "\n",
    "def load_hyper_params(model_dir):\n",
    "    file_name = os.path.join(model_dir, 'hyper_params.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        args = {}\n",
    "        for line in f.readlines():\n",
    "            key, value = line.split('\\t')\n",
    "            args[key.strip()] = str_to_num(value.strip())\n",
    "    print (\"Hyper-Parameters read from {}\".format(file_name))\n",
    "    return args\n",
    "args = load_hyper_params(model_dir)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[(1019, 1019)]\n",
      "{'mygcn/mygraphconvolution_1_vars/weights_0/Adam:0': <tf.Variable 'mygcn/mygraphconvolution_1_vars/weights_0/Adam:0' shape=(24, 20) dtype=float32_ref>, 'mygcn_1/mygraphconvolution_5_vars/weights_0:0': <tf.Variable 'mygcn_1/mygraphconvolution_5_vars/weights_0:0' shape=(20, 40) dtype=float32_ref>, 'mygcn/mygraphconvolution_2_vars/weights_0/Adam_1:0': <tf.Variable 'mygcn/mygraphconvolution_2_vars/weights_0/Adam_1:0' shape=(20, 40) dtype=float32_ref>, 'mygcn_1/mygraphconvolution_4_vars/weights_0:0': <tf.Variable 'mygcn_1/mygraphconvolution_4_vars/weights_0:0' shape=(24, 20) dtype=float32_ref>, 'mygcn/mygraphconvolution_1_vars/weights_0:0': <tf.Variable 'mygcn/mygraphconvolution_1_vars/weights_0:0' shape=(24, 20) dtype=float32_ref>, 'mygcn/mygraphconvolution_1_vars/weights_0/Adam_1:0': <tf.Variable 'mygcn/mygraphconvolution_1_vars/weights_0/Adam_1:0' shape=(24, 20) dtype=float32_ref>, 'mygcn/mygraphconvolution_2_vars/weights_0:0': <tf.Variable 'mygcn/mygraphconvolution_2_vars/weights_0:0' shape=(20, 40) dtype=float32_ref>, 'mygcn/mygraphconvolution_3_vars/weights_0/Adam:0': <tf.Variable 'mygcn/mygraphconvolution_3_vars/weights_0/Adam:0' shape=(40, 2) dtype=float32_ref>, 'mygcn_1/mygraphconvolution_6_vars/weights_0:0': <tf.Variable 'mygcn_1/mygraphconvolution_6_vars/weights_0:0' shape=(40, 2) dtype=float32_ref>, 'mygcn/mygraphconvolution_3_vars/weights_0/Adam_1:0': <tf.Variable 'mygcn/mygraphconvolution_3_vars/weights_0/Adam_1:0' shape=(40, 2) dtype=float32_ref>, 'mygcn/mygraphconvolution_2_vars/weights_0/Adam:0': <tf.Variable 'mygcn/mygraphconvolution_2_vars/weights_0/Adam:0' shape=(20, 40) dtype=float32_ref>, 'mygcn/mygraphconvolution_3_vars/weights_0:0': <tf.Variable 'mygcn/mygraphconvolution_3_vars/weights_0:0' shape=(40, 2) dtype=float32_ref>}\n",
      "INFO:tensorflow:Restoring parameters from ../data/GCN/training/2018_01_22_17_10_15/model.ckpt\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Key mygcn_1/mygraphconvolution_6_vars/weights_0:0 not found in checkpoint\n\t [[Node: save_1/RestoreV2_11 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_11/tensor_names, save_1/RestoreV2_11/shape_and_slices)]]\n\nCaused by op 'save_1/RestoreV2_11', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/roman/.local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/roman/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/roman/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/roman/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-2431f903c446>\", line 36, in <module>\n    model.load(ckpt.model_checkpoint_path, sess)\n  File \"/home/roman/PROJECTS/ppi/legionella/GCN/my_gcn.py\", line 166, in load\n    saver = tf.train.Saver(self.vars)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1139, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1170, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 640, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Key mygcn_1/mygraphconvolution_6_vars/weights_0:0 not found in checkpoint\n\t [[Node: save_1/RestoreV2_11 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_11/tensor_names, save_1/RestoreV2_11/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key mygcn_1/mygraphconvolution_6_vars/weights_0:0 not found in checkpoint\n\t [[Node: save_1/RestoreV2_11 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_11/tensor_names, save_1/RestoreV2_11/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2431f903c446>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMYGCN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplaceholders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PROJECTS/ppi/legionella/GCN/my_gcn.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, sess)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model restored from file: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1546\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1547\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1548\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key mygcn_1/mygraphconvolution_6_vars/weights_0:0 not found in checkpoint\n\t [[Node: save_1/RestoreV2_11 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_11/tensor_names, save_1/RestoreV2_11/shape_and_slices)]]\n\nCaused by op 'save_1/RestoreV2_11', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/roman/.local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/roman/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/roman/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/roman/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-2431f903c446>\", line 36, in <module>\n    model.load(ckpt.model_checkpoint_path, sess)\n  File \"/home/roman/PROJECTS/ppi/legionella/GCN/my_gcn.py\", line 166, in load\n    saver = tf.train.Saver(self.vars)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1139, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1170, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 640, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Key mygcn_1/mygraphconvolution_6_vars/weights_0:0 not found in checkpoint\n\t [[Node: save_1/RestoreV2_11 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_11/tensor_names, save_1/RestoreV2_11/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "adj = csr_matrix(network)\n",
    "F = lil_matrix(features)\n",
    "F = preprocess_features(F)\n",
    "\n",
    "poly_support = args['support']\n",
    "if poly_support > 1:\n",
    "    support = chebyshev_polynomials(adj, poly_support)\n",
    "    num_supports = 1 + poly_support\n",
    "else:\n",
    "    support = [preprocess_adj(adj)]\n",
    "    num_supports = 1\n",
    "print (len(support))\n",
    "print ([s[2] for s in support])\n",
    "\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32,\n",
    "                                      shape=tf.constant(features.shape, dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, 2)),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}\n",
    "\n",
    "def predict(features, support, labels, mask, placeholders):\n",
    "    feed_dict_pred = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    pred = sess.run(model.predict(), feed_dict=feed_dict_pred)\n",
    "    return pred\n",
    "\n",
    "ckpt = tf.train.get_checkpoint_state(checkpoint_dir=model_dir)\n",
    "ckpt.model_checkpoint_path\n",
    "\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    with tf.Session() as sess:\n",
    "        model = MYGCN(placeholders, input_dim=features.shape[1], logging=True)\n",
    "        model.load(ckpt.model_checkpoint_path, sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the predictions have special properties?\n",
    "I want to check the shortest path between predicted disease genes and true disease genes as well.\n",
    "Further, I want to see them in a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build networkx graph with correct node names\n",
    "G = nx.from_numpy_matrix(network)\n",
    "#mapping = {i:node_names[i, 0] for i in range(node_names.shape[0])}\n",
    "#nx.relabel_nodes(G, mapping, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortest Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate shortest paths between all nodes and make it a dataframe matrix\n",
    "all_shortest_paths = nx.shortest_path_length(G)\n",
    "sp_df = pd.DataFrame(dict(all_shortest_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build views for label to label shortest paths and non-label to non-label\n",
    "label_cond = sp_df.index.isin(predictions_for_knowns.index)\n",
    "label_to_label = sp_df.loc[label_cond, label_cond]\n",
    "nonlabel_to_nonlabel = sp_df.loc[~label_cond, ~label_cond]\n",
    "label_to_nonlabel = sp_df.loc[~label_cond, label_cond]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distributions\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "bins = np.arange(0, 8, 1)\n",
    "\n",
    "ltl = pd.Series(label_to_label.values.flatten()).dropna()\n",
    "ntn = pd.Series(nonlabel_to_nonlabel.values.flatten()).dropna()\n",
    "ltn = pd.Series(label_to_nonlabel.values.flatten()).dropna()\n",
    "print (ltl.shape, ntn.shape, ltn.shape)\n",
    "plt.hist(ltl, bins=bins, alpha=.5, normed=True, color='red',\n",
    "         label='Infection To Infection (mean {0:.2f})'.format(ltl.mean()))\n",
    "plt.hist(ntn, bins=bins, alpha=.7, normed=True, color='gray',\n",
    "         label='Non-Infection To Non-Infection (mean {0:.2f})'.format(ntn.mean()))\n",
    "plt.hist(ltn, bins=bins, alpha=.3, normed=True, color='blue',\n",
    "         label='Infection To Non-Infection (mean {0:.2f})'.format(ltn.mean()))\n",
    "points = np.linspace(0, 8, 100)\n",
    "plt.plot(points, mlab.normpdf(points, ltl.mean(), ltl.std()), color='red', alpha=0.5)\n",
    "plt.plot(points, mlab.normpdf(points, ntn.mean(), ntn.std()), color='gray', alpha=0.5)\n",
    "plt.plot(points, mlab.normpdf(points, ltn.mean(), ltn.std()), color='blue', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title('Histograms of Shortest Paths between Infection Genes and Non-Infection Genes')\n",
    "plt.xlabel('Shortest Path Length')\n",
    "plt.ylabel('Normalized Frequency')\n",
    "fig.savefig(model_dir + '/path_length_distribution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate shortest paths between all nodes and make it a dataframe matrix\n",
    "all_node_degrees = G.degree()\n",
    "degree_df = pd.Series(dict(all_node_degrees))\n",
    "\n",
    "# build views for label and non-label degrees\n",
    "label_cond = degree_df.index.isin(predictions_for_knowns.index)\n",
    "labels = degree_df[label_cond]\n",
    "nonlabels = degree_df[~label_cond]\n",
    "print (labels.shape, nonlabels.shape)\n",
    "\n",
    "# plot histograms\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "bins = np.arange(0, 50, 1)\n",
    "plt.hist(labels, bins=bins, alpha=.5, normed=True, color='red',\n",
    "         label='Infection Genes (Median {})'.format(labels.median()))\n",
    "plt.hist(nonlabels, bins=bins, alpha=.7, normed=True, color='gray',\n",
    "         label='Other Genes (Median {})'.format(nonlabels.median()))\n",
    "plt.legend()\n",
    "plt.title('Histograms of Node Degree for Infection and Other Genes')\n",
    "plt.xlabel('Node Degree')\n",
    "plt.ylabel('Normalized Frequency')\n",
    "fig.savefig(model_dir + 'node_degrees.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability that Neighbor of Infection Gene is also Infection Gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# half because of symmetric matrix\n",
    "num_of_neighbor_labels = label_to_label[label_to_label == 1].count().sum() / 2\n",
    "prob_neighbor_label = (num_of_neighbor_labels / label_to_label.shape[0]) * 100\n",
    "print (\"Chance of label to have labeled neighbor: {0:.2f} %\".format(prob_neighbor_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_label_nbs = []\n",
    "num_nbs = []\n",
    "for label in predictions.index:\n",
    "    nbs = list(nx.all_neighbors(G, label))\n",
    "    label_nbs = predictions_for_knowns.index.isin(nbs).sum()\n",
    "    num_label_nbs.append(label_nbs)\n",
    "    num_nbs.append(len(nbs))\n",
    "num_label_nbs = np.array(num_label_nbs)\n",
    "num_total_nbs = np.array(num_nbs)\n",
    "print (\"Number of labeled neighbors: {}\".format(num_label_nbs.sum()))\n",
    "print (\"Number of neighbors total: {}\".format(num_total_nbs.sum()))\n",
    "freq = num_label_nbs.sum()/num_total_nbs.sum()\n",
    "print (\"Chance that neighbor is labeled: {0:.4f}\".format(freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 150, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 40, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_float('dropout', 0.6, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 50, 'Tolerance for early stopping (# of epochs).')\n",
    "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
    "flags.DEFINE_bool('cheby', True, 'Using Chebyshev convolutions or not.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = csr_matrix(network)\n",
    "F = lil_matrix(features)\n",
    "F = preprocess_features(F)\n",
    "support = chebyshev_polynomials(adj, 2)\n",
    "print (len(support))\n",
    "print ([s[2] for s in support])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(1+FLAGS.max_degree)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features.shape, dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, 2)),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}\n",
    "\n",
    "def predict(features, support, labels, mask, placeholders):\n",
    "    feed_dict_pred = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    pred = sess.run(model.predict(), feed_dict=feed_dict_pred)\n",
    "    return pred\n",
    "\n",
    "weights_layer_0 = []\n",
    "weights_layer_1 = []\n",
    "with tf.Session() as sess:\n",
    "    model = GCN(placeholders, input_dim=features.shape[1], logging=True)\n",
    "    model.load(sess)\n",
    "    for k in range(FLAGS.max_degree): # chebychev coefficients\n",
    "        mat_name = 'gcn/graphconvolution_{}_vars/weights_{}:0'.format(1, k)\n",
    "        weights_layer_0.append(model.vars[mat_name].eval())\n",
    "        mat_name = 'gcn/graphconvolution_{}_vars/weights_{}:0'.format(2, k)\n",
    "        weights_layer_1.append(model.vars[mat_name].eval())\n",
    "    \n",
    "    # predict for test set and catch activations in H_1, H_2\n",
    "    feed_dict = construct_feed_dict(features=F,\n",
    "                                    support=support,\n",
    "                                    labels=y_train,\n",
    "                                    labels_mask=train_mask,\n",
    "                                    placeholders=placeholders\n",
    "                                   )\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    #l = sess.run(model.loss, feed_dict=feed_dict)\n",
    "    activation_0 = sess.run(model.activations[-2], feed_dict=feed_dict)\n",
    "    activation_1 = sess.run(model.activations[-1], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights_for_k(matrix, fname='kernel_activation'):\n",
    "    fig = plt.figure(figsize=(14, 8))\n",
    "    num_rows, num_cols = bestSplit(matrix.shape[1])\n",
    "    for i in range(matrix.shape[1]):\n",
    "        plt.subplot(num_rows, num_cols, i+1)\n",
    "        plt.bar(np.arange(0, matrix.shape[0]), matrix[:, i])\n",
    "    fig.savefig('{}/{}.png'.format(model_dir, fname))\n",
    "\n",
    "weights_l0_sum = np.sum(weights_layer_0, axis=0)\n",
    "print (weights_l0_sum.shape)\n",
    "plot_weights_for_k(weights_l0_sum, 'filters_layer0_k0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_l1_sum = np.sum(weights_layer_1, axis=0)\n",
    "plot_weights_for_k(weights_l1_sum, 'filters_layer1_sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the activations\n",
    "Next, I want to see if the activations cluster together, that is if the output from each of the layers is embedded nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_and_labels = labels_df.join(predictions)\n",
    "pos_classified = preds_and_labels.Prob_pos >= CLASSIFICATION_THRESHOLD\n",
    "\n",
    "# set the colors according to classes\n",
    "preds_and_labels['color'] = 'gray'\n",
    "preds_and_labels.loc[pos_classified, 'color'] = 'red'\n",
    "preds_and_labels.loc[~preds_and_labels.label & pos_classified, 'color'] = 'green'\n",
    "preds_and_labels.groupby('color').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_1_2d = TSNE(n_components=2).fit_transform(activation_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 8))\n",
    "plt.scatter(H_1_2d[:, 0], H_1_2d[:, 1], c=preds_and_labels.color, alpha=0.7)\n",
    "plt.xlabel('TSNE Component 1')\n",
    "plt.ylabel('TSNE Component 2')\n",
    "plt.title('TSNE Plot: First Hidden Layer')\n",
    "\n",
    "# legend\n",
    "inf_genes = mpatches.Patch(color='red', label='Infection Gene')\n",
    "pred_genes = mpatches.Patch(color='green', label='Predicted Infection Gene')\n",
    "not_involved = mpatches.Patch(color='gray', label='Not Involved in Infection')\n",
    "plt.legend(handles=[inf_genes, pred_genes])\n",
    "\n",
    "# save\n",
    "fig.savefig(model_dir + '/tsne_H1.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_2_2d = TSNE(n_components=2).fit_transform(activation_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 8))\n",
    "plt.scatter(H_2_2d[:, 0], H_2_2d[:, 1], c=preds_and_labels.color, alpha=.7)\n",
    "plt.xlabel('TSNE Component 1')\n",
    "plt.ylabel('TSNE Component 2')\n",
    "plt.title('TSNE Plot: Second Hidden Layer')\n",
    "# legend\n",
    "inf_genes = mpatches.Patch(color='red', label='Infection Gene')\n",
    "pred_genes = mpatches.Patch(color='green', label='Predicted Infection Gene')\n",
    "plt.legend(handles=[inf_genes, pred_genes])\n",
    "# save\n",
    "fig.savefig(model_dir + '/tsne_H2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_1_pca = PCA(n_components=2).fit_transform(activation_0)\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "plt.scatter(H_1_pca[:, 0], H_1_pca[:, 1], c=preds_and_labels.color, alpha=.7)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA Plot: First Hidden Layer')\n",
    "# legend\n",
    "inf_genes = mpatches.Patch(color='red', label='Infection Gene')\n",
    "pred_genes = mpatches.Patch(color='green', label='Predicted Infection Gene')\n",
    "plt.legend(handles=[inf_genes, pred_genes])\n",
    "# save\n",
    "fig.savefig(model_dir + '/pca_H1.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_2_pca = PCA(n_components=2).fit_transform(activation_1)\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "plt.scatter(H_2_pca[:, 0], H_2_pca[:, 1], c=preds_and_labels.color, alpha=.7)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA Plot: Second Hidden Layer')\n",
    "# legend\n",
    "inf_genes = mpatches.Patch(color='red', label='Infection Gene')\n",
    "pred_genes = mpatches.Patch(color='green', label='Predicted Infection Gene')\n",
    "plt.legend(handles=[inf_genes, pred_genes])\n",
    "# save\n",
    "fig.savefig(model_dir + '/pca_H2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation with other methods\n",
    "I will now try to see how similar my performance is to NetRank and PageRank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netrank_scores = pd.DataFrame.from_csv('../data/pagerank/netrank_scores.txt', header=0, sep='\\t')\n",
    "netrank_scores.rename(columns={'Rank': 'NetRank_Rank'})\n",
    "predictions.sort_values('Prob_pos', ascending=False, inplace=True)\n",
    "predictions['GCN_Rank'] = np.arange(1, predictions.shape[0]+1)\n",
    "print (\"Correlation Rank & Prob: {}\".format(predictions.Prob_pos.corr(predictions.GCN_Rank)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netrank_with_predictions = predictions.join(netrank_scores, how='inner')\n",
    "fig = plt.figure(figsize=(14, 14))\n",
    "plt.scatter(netrank_with_predictions.GCN_Rank, netrank_with_predictions.Rank, color='gray')\n",
    "lin = np.arange(1, netrank_with_predictions.shape[0]+1)\n",
    "plt.plot(lin, lin, '--', color='black')\n",
    "\n",
    "print (\"Correlation between NetRank and GCN: {}\".format(netrank_with_predictions.Rank.corr(netrank_with_predictions.GCN_Rank)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netrank_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
